
### 方案一：规则引擎 + QT 辅助  
**（最快落地，1–2 周可见原型）**  
这是成本最低、效果最直观的方案。先不折腾复杂的模型训练，而是先搭建起“数据通路”。  

**技术栈：**  
spirecomm + Bottled AI（逻辑参考）+ PySide6（QT）  

**实现逻辑：**  
1. 使用 spirecomm 获取当前游戏状态（手牌、能量、怪物意图）。  
2. 参考 Bottled AI 中的启发式算法（例如：每张牌有基础分，根据当前状态加减分）。  
3. QT 界面：做一个透明置顶窗口，实时显示每张手牌的“得分”和“推荐操作”。  

**优点：**  
- 逻辑可控，调试极其方便  
- 不需要高性能显卡跑训练  

**缺点：**  
- 策略上限受限于你写的规则  

---

### 方案二：预训练模型 + QT 联动  
**（平衡方案，3–4 周）**  
如果你希望项目看起来更有“AI 感”，可以直接借用社区已有的 RL 成果。  

**技术栈：**  
spirecomm + slai-the-spire（模型部分）+ PySide6  

**实现逻辑：**  
1. 利用 slai-the-spire 项目中已经定义好的 Observation Space（特征工程）。  
2. 直接加载该项目提供的预训练权重（DQN 模型）。  
3. 集成：当 spirecomm 拿到状态后，将其向量化输入模型，得到 Q 值（动作评价），QT 界面展示 Q 值最高的动作。  

**优点：**  
- 真正意义上的强化学习落地，能够处理复杂的卡牌连锁  

**缺点：**  
- slai-the-spire 的模型特征维度复杂，代码适配需要一定功底  

---

### 方案三：自研轻量级 RL  
**（学术 / 深造方案，1 个月以上）**  
如果你是为了学习 RL 全过程，或者想做一个完全属于自己的 AI。  

**技术栈：**  
spirecomm + Stable Baselines3 + Gymnasium  

**实现逻辑：**  
1. 封装环境：使用 spirecomm 作为底层，按照 OpenAI Gym 标准封装一个 SpsEnv。  
2. 简化状态：初期只给 AI 输入简单的特征（如：血量、能量、攻击牌数量）。  
3. 训练与部署：训练完成后，导出 .zip 或 .onnx 模型文件，供 QT 界面调用。  

**优点：**  
- 整个链路完全自主可控，论文或项目演示价值极高  

**缺点：**  
- 训练非常耗时，且容易出现 AI 怎么学也学不会（不收敛）的情况
